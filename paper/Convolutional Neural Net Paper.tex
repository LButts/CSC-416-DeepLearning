\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xspace}

\title{Applying a Convolutional Neural Net to Street View House Numbers}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Levi Butts\\
	CENG Undergraduate\\
	South Dakota School of Mines and Technology\\
	Rapid City, SD 57701 \\
	\texttt{Levi.Butts@mines.sdsmt.edu} \\
}

\newcommand{\MATLAB}{\textsc{Matlab}\xspace}

\begin{document}
	% \nipsfinalcopy is no longer used
	
	\maketitle
	
	\begin{abstract}
		The abstract paragraph should be indented \nicefrac{1}{2}~inch
		(3~picas) on both the left- and right-hand margins. Use 10~point
		type, with a vertical spacing (leading) of 11~points.  The word
		\textbf{Abstract} must be centered, bold, and in point size 12. Two
		line spaces precede the abstract. The abstract must be limited to
		one paragraph.
	\end{abstract}
	
	\section{Introduction}
	
	Properly identifying text characters from photographs, in a timely manner, is one of the quintessential problems that machine learning programs aim to solve.  While the classification of controlled and restricted datasets is by and large a solved problem (i.e. recognizing handwritten digits, or characters in printed documents), performing similar tasks on images of natural scenes becomes much more difficult.  Natural phenomena: severe blur, distortion, illumination effects, etc. when coupled with the wide variation of font styles and display methods, are incredibly difficult to compensate for by hand.  A neural network operating on a learned feature-based classification method, such as image convolution, harbors plenty of potential to adequately handle this problem.   Utilizing a benchmark dataset captured from Google Street View images, which consists of over 600,000 digit images, this work will demonstrate the effectiveness of learned feature classification.
	
	Character recognition in images is a well studied, and well documented problem, with years of focused research performed by both academia and industry.  The MNIST digit dataset has been so thoroughly addressed that now it is used mainly as a jumping off point for machine learning beginners.  Any number of tutorials for the MNIST dataset can be found that produce near perfect accuracy with 'off-the-shelf' algorithms.  
	
	For this paper, the main focus will rest on taking the well-honed techniques used for the MNIST dataset and applying them to a restricted instance of scene-text recognition.  Namely the classification of house number signs in street level images from the Google Street View House Numbers (SVHN) dataset.  As stated previously, MNIST has been a valued standard for researchers seeking to improve their learning systems to compare against.  The SVHN dataset can be noted to have a style similar to MNIST, but SVHN comprises itself of data that comes from a significantly harder real world problem, and contains an order of magnitude more data than MNIST.
	
	To start, I will walk through details of the SVHN dataset in Section \ref{dt_set}, as well as the associated task of preparing and pre-processing required to properly interface the data with Tensorflow in Section \ref{pre-proc}.  Section \ref{model} will be an overview of the model design and the associated results will be discussed in Section \ref{results}.  All of this should adequately convey the power associated with feature based classification executed via a convolutional neural net.
	
	\section{Street View House Numbers (SVHN) Dataset}
	\label{dt_set}
	
	The SVHN dataset was compiled from a large number of Street View images from urban areas in various countries.  The house-numbers were extracted via a sliding window detector (cite source).  The dataset was then made available in two formats:
	
	\begin{itemize}
		\item Full Numbers - these are the original, variable resolution, color house-number images. Each image has character level bounding boxes and a transcription of the detected digits.
		
		\item Cropped Digits - these are formatted versions of the original images to resemble the MNIST set. Each image has been cropped and resized to be a fixed resolution of 32-by-32 pixels with the digit centered.
		  
	\end{itemize}
	
	As is explained in (cite source here), the original collection of images for SVHN were compiled with the intent of using an end-to-end system that first detects the location of a house number in a larger image, then uses a detection stage to classify each digit (0 through 9).  For the purposes of this work I utilized the more controlled version of cropped images.  The cropped images were selected because the task of locating, identifying and processing the original images is a separate problem in and of itself.
	
	The dataset is divided into a total of three subsets:
	
	\begin{itemize}
		
		\item SVHN train - 73,257 digits for training
		\item SVHN test - 26,032 digits for testing
		\item SVHN extra - 531,131 additional, marginally less difficult samples, to use as extra training data
		
	\end{itemize}

	The first 2 subsets were obtained from a large number of Street View images. Even though SVHN-extra was generated in a similar manner, it is generally easier than SVHN-train and SVHN-test as a result of changes made to generate a dataset of such a large size.
	
	
	\section{Data Pre-processing}
	\label{pre-proc}
		
	The SVHN dataset is available for free on the Internet and can be found at
	\begin{center}
		\url{http://ufldl.stanford.edu/housenumbers/}
	\end{center}
	where both the full-number and cropped-digit versions are available.
	
	The cropped-digits are saved as \MATLAB arrays inside of their respective .mat files. Properly interpreting the \MATLAB files was a substantial issue at the outset as they are parsed fairly differently from things like CSV, and IDX.  The solution for interpreting the .mat files was to convert them into .npy files consisting of numpy arrays. One common part of pre-processing is converting the labels int a vector of one-hot vectors; however, for this application it was decided not to do any one-hot encoding 
	to interface correctly with the Tensorflow loss function.

	\section{Model Design}
	\label{model}
	
	These instructions apply to everyone.
	
	\subsection{Citations within the text}
	
	The \verb+natbib+ package will be loaded for you by default.
	Citations may be author/year or numeric, as long as you maintain
	internal consistency.  As to the format of the references themselves,
	any style is acceptable as long as it is used consistently.
	
	The documentation for \verb+natbib+ may be found at
	\begin{center}
		\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
	\end{center}
	Of note is the command \verb+\citet+, which produces citations
	appropriate for use in inline text.  For example,
	\begin{verbatim}
	\citet{hasselmo} investigated\dots
	\end{verbatim}
	produces
	\begin{quote}
		Hasselmo, et al.\ (1995) investigated\dots
	\end{quote}
	
	If you wish to load the \verb+natbib+ package with options, you may
	add the following before loading the \verb+nips_2017+ package:
	\begin{verbatim}
	\PassOptionsToPackage{options}{natbib}
	\end{verbatim}
	
	If \verb+natbib+ clashes with another package you load, you can add
	the optional argument \verb+nonatbib+ when loading the style file:
	\begin{verbatim}
	\usepackage[nonatbib]{nips_2017}
	\end{verbatim}
	
	As submission is double blind, refer to your own published work in the
	third person. That is, use ``In the previous work of Jones et
	al.\ [4],'' not ``In our previous work [4].'' If you cite your other
	papers that are not widely available (e.g., a journal paper under
	review), use anonymous author names in the citation, e.g., an author
	of the form ``A.\ Anonymous.''
	
	\subsection{Footnotes}
	
	Footnotes should be used sparingly.  If you do require a footnote,
	indicate footnotes with a number\footnote{Sample of the first
		footnote.} in the text. Place the footnotes at the bottom of the
	page on which they appear.  Precede the footnote with a horizontal
	rule of 2~inches (12~picas).
	
	Note that footnotes are properly typeset \emph{after} punctuation
	marks.\footnote{As in this example.}
	
	\subsection{Figures}
	
	All artwork must be neat, clean, and legible. Lines should be dark
	enough for purposes of reproduction. The figure number and caption
	always appear after the figure. Place one line space before the figure
	caption and one line space after the figure. The figure caption should
	be lower case (except for first word and proper nouns); figures are
	numbered consecutively.
	
	You may use color figures.  However, it is best for the figure
	captions and the paper body to be legible if the paper is printed in
	either black/white or in color.
	\begin{figure}[h]
		\centering
		\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\caption{Sample figure caption.}
	\end{figure}
	
	\subsection{Tables}
	
	All tables must be centered, neat, clean and legible.  The table
	number and title always appear before the table.  See
	Table~\ref{sample-table}.
	
	Place one line space before the table title, one line space after the
	table title, and one line space after the table. The table title must
	be lower case (except for first word and proper nouns); tables are
	numbered consecutively.
	
	Note that publication-quality tables \emph{do not contain vertical
		rules.} We strongly suggest the use of the \verb+booktabs+ package,
	which allows for typesetting high-quality, professional tables:
	\begin{center}
		\url{https://www.ctan.org/pkg/booktabs}
	\end{center}
	This package was used to typeset Table~\ref{sample-table}.
	
	\begin{table}[t]
		\caption{Sample table title}
		\label{sample-table}
		\centering
		\begin{tabular}{lll}
			\toprule
			\multicolumn{2}{c}{Part}                   \\
			\cmidrule{1-2}
			Name     & Description     & Size ($\mu$m) \\
			\midrule
			Dendrite & Input terminal  & $\sim$100     \\
			Axon     & Output terminal & $\sim$10      \\
			Soma     & Cell body       & up to $10^6$  \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\section{Results}
	\label{results}
	
	Do not change any aspects of the formatting parameters in the style
	files.  In particular, do not modify the width or length of the
	rectangle the text should fit into, and do not change font sizes
	(except perhaps in the \textbf{References} section; see below). Please
	note that pages should be numbered.
	
	
	\section{Conclusion}
	\label{conc}
	
	\section{Acknowledgment}
	
	Please prepare submission files with paper size ``US Letter,'' and
	not, for example, ``A4.''
	
	Fonts were the main cause of problems in the past years. Your PDF file
	must only contain Type 1 or Embedded TrueType fonts. Here are a few
	instructions to achieve this.
	
	\begin{itemize}
		
		\item You should directly generate PDF files using \verb+pdflatex+.
		
		\item You can check which fonts a PDF files uses.  In Acrobat Reader,
		select the menu Files$>$Document Properties$>$Fonts and select Show
		All Fonts. You can also use the program \verb+pdffonts+ which comes
		with \verb+xpdf+ and is available out-of-the-box on most Linux
		machines.
		
		\item The IEEE has recommendations for generating PDF files whose
		fonts are also acceptable for NIPS. Please see
		\url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}
		
		\item \verb+xfig+ "patterned" shapes are implemented with bitmap
		fonts.  Use "solid" shapes instead.
		
		\item The \verb+\bbold+ package almost always uses bitmap fonts.  You
		should use the equivalent AMS Fonts:
		\begin{verbatim}
		\usepackage{amsfonts}
		\end{verbatim}
		followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or
		\verb+\mathbb{C}+ for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You
		can also use the following workaround for reals, natural and complex:
		\begin{verbatim}
		\newcommand{\RR}{I\!\!R} %real numbers
		\newcommand{\Nat}{I\!\!N} %natural numbers
		\newcommand{\CC}{I\!\!\!\!C} %complex numbers
		\end{verbatim}
		Note that \verb+amsfonts+ is automatically loaded by the
		\verb+amssymb+ package.
		
	\end{itemize}
	
	If your file contains type 3 fonts or non embedded TrueType fonts, we
	will ask you to fix it.
	
	\subsection{Margins in \LaTeX{}}
	
	Most of the margin problems come from figures positioned by hand using
	\verb+\special+ or other commands. We suggest using the command
	\verb+\includegraphics+ from the \verb+graphicx+ package. Always
	specify the figure width as a multiple of the line width as in the
	example below:
	\begin{verbatim}
	\usepackage[pdftex]{graphicx} ...
	\includegraphics[width=0.8\linewidth]{myfile.pdf}
	\end{verbatim}
	See Section 4.4 in the graphics bundle documentation
	(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})
	
	A number of width problems arise when \LaTeX{} cannot properly
	hyphenate a line. Please give LaTeX hyphenation hints using the
	\verb+\-+ command when necessary.
	
	\subsubsection*{Acknowledgments}
	
	Use unnumbered third level headings for the acknowledgments. All
	acknowledgments go at the end of the paper. Do not include
	acknowledgments in the anonymized submission, only in the final paper.
	
	\section*{References}
	
	References follow the acknowledgments. Use unnumbered first-level
	heading for the references. Any choice of citation style is acceptable
	as long as you are consistent. It is permissible to reduce the font
	size to \verb+small+ (9 point) when listing the references. {\bf
		Remember that you can go over 8 pages as long as the subsequent ones contain
		\emph{only} cited references.}
	\medskip
	
	\small
	
	[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
	for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
	T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
		Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.
	
	[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
		Exploring Realistic Neural Models with the GEneral NEural SImulation
		System.}  New York: TELOS/Springer--Verlag.
	
	[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
	learning and recall at excitatory recurrent synapses and cholinergic
	modulation in rat hippocampal region CA3. {\it Journal of
		Neuroscience} {\bf 15}(7):5249-5262.
	
\end{document}